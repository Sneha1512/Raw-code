{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[('NN', 994876), ('IN', 379166), ('JJ', 302062), ('NNS', 293766), ('DT', 283586)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['zipf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEACAYAAACkvpHUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ae0b310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",             125824\n",
      ".             122420\n",
      "****           32532\n",
      ":              25504\n",
      "experience     24920\n",
      "work           18209\n",
      "role           16962\n",
      "team           16298\n",
      "client         16251\n",
      "care           14246\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "import csv\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "def preprocessing():\n",
    "    job_descriptions = pd.read_csv('Train_15000.csv')['FullDescription']\n",
    "    # Tokenize\n",
    "    words = [w.lower() for job_description in job_descriptions for w in nltk.word_tokenize(job_description.decode(\"ascii\", \"ignore\"))]\n",
    "    return words\n",
    "\n",
    "def words():\n",
    "    words = preprocessing()    \n",
    "    words_tagged = nltk.pos_tag(words)\n",
    "    tags = [b for (a, b) in words_tagged]\n",
    "\n",
    "    # Count each part-of-speech\n",
    "    tag_dict = {}\n",
    "    for tag in set(tags):\n",
    "        tag_dict[tag] = 0\n",
    "\n",
    "    for tag in tags:\n",
    "        tag_dict[tag] += 1\n",
    "    sorted_tag = sorted(tag_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    print sorted_tag[:5]\n",
    "\n",
    "\n",
    "def zipf():\n",
    "    text_normal = preprocessing()\n",
    "    word_freq = pd.Series(text_normal).value_counts()\n",
    "    plt.plot(word_freq[0:100].values)\n",
    "    plt.show()\n",
    "\n",
    "def processed():\n",
    "    text_normal = preprocessing()\n",
    "\n",
    "    # remove stopwords\n",
    "    no_stopword = [word for word in text_normal if word not in stopwords.words('english')]\n",
    "\n",
    "    # lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in no_stopword]\n",
    "\n",
    "    word_freq = pd.Series(lemmatized).value_counts()\n",
    "    return word_freq[0:10]\n",
    "\n",
    "words()\n",
    "zipf()\n",
    "print(processed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Multinomial prediction:\n",
      "(array([[3643,  790],\n",
      "       [ 804, 3544]]), 0.81847170026192917)\n",
      "Multinomial prediction with lemmatization:\n",
      "(array([[3624,  809],\n",
      "       [ 861, 3487]]), 0.80981664958432975)\n",
      "Multinomial prediction without stopwords:\n",
      "(array([[3642,  791],\n",
      "       [ 833, 3515]]), 0.81505523288919257)\n",
      "top 10 indicative words for low salary class:\n",
      "experience   -4.682014\n",
      "nan          -4.696368\n",
      "the          -4.871965\n",
      "work         -4.903452\n",
      "uk           -4.904317\n",
      "care         -4.941719\n",
      "jobs         -5.015048\n",
      "role         -5.027693\n",
      "client       -5.092689\n",
      "team         -5.132883\n",
      "dtype: float64\n",
      "top 10 indicative words for high salary class:\n",
      "experience   -4.466887\n",
      "team         -4.867182\n",
      "the          -4.873850\n",
      "nan          -4.908411\n",
      "uk           -4.914706\n",
      "manager      -4.915796\n",
      "business     -4.943985\n",
      "work         -5.007754\n",
      "role         -5.011106\n",
      "client       -5.038455\n",
      "dtype: float64\n",
      "None\n",
      "Multinomial prediction with bigram:\n",
      "(array([[3630,  803],\n",
      "       [ 793, 3555]]), 0.81824393577041343)\n",
      "Multinomial prediction with tf-idf:\n",
      "(array([[3551,  882],\n",
      "       [ 679, 3669]]), 0.82222981437193943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['zipf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%pylab inline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.random as npr\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "### PART B preparation\n",
    "\n",
    "# drop non-string fields in df, combine each row into one string\n",
    "def str_text():\n",
    "    # use pandas read_csv instead of basic form read()!\n",
    "    text_df = pd.read_csv('Train_15000.csv')\n",
    "    # drop SalaryNormalized and save it in a new variable\n",
    "    salary = text_df.pop('SalaryNormalized')\n",
    "    # drop columns we don't want\n",
    "    text_df = text_df.drop('Id', axis=1)\n",
    "    text_df = text_df.drop('SalaryRaw', axis=1)\n",
    "    # write a function to convert a row into a string\n",
    "    def to_str(row):\n",
    "        return ' '.join(pd.Series(row.values).map(str))\n",
    "    text_str = text_df.apply(to_str, axis=1)\n",
    "    text_str = pd.Series([text.decode('ascii', 'ignore') for text in text_str])\n",
    "    return text_str, salary\n",
    "\n",
    "    \n",
    "\n",
    "# B1 -- basic multinomial bayes prediction\n",
    "def prediction():\n",
    "    text_str = str_text()[0]\n",
    "    # get the bag of words\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(text_str)\n",
    "    X = X.toarray()\n",
    "\n",
    "    # mask the salary into high (75 percentile and above) and low (below that)\n",
    "    salary = str_text()[1]\n",
    "    cutoff = salary.quantile(q=.75)\n",
    "    y = (salary >= cutoff)\n",
    "    \n",
    "    # resample the high salary to make equal sample of high and low salary\n",
    "    n = len(y[y==0])\n",
    "    index_high = [i for i in y.index if y[i]]\n",
    "    index_low = [i for i in y.index if not y[i]]\n",
    "    new_high = npr.choice(index_high, size = n, replace = True)\n",
    "    high_X = X[new_high]\n",
    "    high_y = y[new_high]\n",
    "    low_X = X[index_low]\n",
    "    low_y = y[index_low]\n",
    "    new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "    new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "    \n",
    "\n",
    "    # split into training and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # fit the model and predict\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Multinomial prediction:')\n",
    "    return conf_matrix, accuracy\n",
    "\n",
    "# B2: bayes with lemmatization, B3: bayes without stopwords\n",
    "def prediction_lemma():\n",
    "    text_str = str_text()[0]\n",
    "\n",
    "    # use lemmatization. My guess is it will improve the result but not much, since 92% is already very good.\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # first split rows into tokens and lemmatize each of them\n",
    "    def lemma(row):\n",
    "        text_token = nltk.word_tokenize(row)\n",
    "        return [wordnet_lemmatizer.lemmatize(word) for word in text_token]\n",
    "    lemmatized = text_str.apply(lemma)\n",
    "    # and then merge tokens back into strings, and then put strings into a series, because fit_transform() only takes series\n",
    "    lemmatized_str = pd.Series([' '.join(row) for row in lemmatized])\n",
    "\n",
    "    # construct X sparse matrix as usual\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(lemmatized_str)\n",
    "    X = X.toarray()\n",
    "\n",
    "    # construct y vector as usual. mask the salary into high (75 percentile and above) and low (below that)\n",
    "    salary = str_text()[1]\n",
    "    cutoff = salary.quantile(q=.75)\n",
    "    y = (salary >= cutoff)\n",
    "\n",
    "    # resample the high salary to make equal sample of high and low salary\n",
    "    n = len(y[y==0])\n",
    "    index_high = [i for i in y.index if y[i]]\n",
    "    index_low = [i for i in y.index if not y[i]]\n",
    "    new_high = npr.choice(index_high, size = n, replace = True)\n",
    "    high_X = X[new_high]\n",
    "    high_y = y[new_high]\n",
    "    low_X = X[index_low]\n",
    "    low_y = y[index_low]\n",
    "    new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "    new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "    \n",
    "    # split into training and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # fit the model and predict\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Multinomial prediction with lemmatization:')\n",
    "    print(conf_matrix, accuracy)\n",
    "\n",
    "    # continue with removing stopwords. B3.\n",
    "    def stop(row):\n",
    "        return [word for word in row if word not in stopwords.words('english')]\n",
    "    no_stopword = lemmatized.map(stop)\n",
    "    no_stopword_str = pd.Series([' '.join(row) for row in no_stopword])\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(no_stopword_str)\n",
    "    X = X.toarray()\n",
    "    \n",
    "    # resample the high salary to make equal sample of high and low salary\n",
    "    n = len(y[y==0])\n",
    "    index_high = [i for i in y.index if y[i]]\n",
    "    index_low = [i for i in y.index if not y[i]]\n",
    "    new_high = npr.choice(index_high, size = n, replace = True)\n",
    "    high_X = X[new_high]\n",
    "    high_y = y[new_high]\n",
    "    low_X = X[index_low]\n",
    "    low_y = y[index_low]\n",
    "    new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "    new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "\n",
    "    # split into training and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # fit the model and predict\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # removing stopwords slightly hurt the result.\n",
    "    print('Multinomial prediction without stopwords:')\n",
    "    print(conf_matrix, accuracy)\n",
    "\n",
    "    # show the top 10 indicative words\n",
    "    # construct a series with negative class (low salary) features and probabilities, sort it, get the first 10\n",
    "    no_prob = pd.Series(data=clf.feature_log_prob_[0], index=vectorizer.get_feature_names())\n",
    "    no_prob.sort(ascending=False)\n",
    "    print('top 10 indicative words for low salary class:')\n",
    "    print(no_prob[:10])\n",
    "\n",
    "    # high salary features\n",
    "    yes_prob = pd.Series(data=clf.feature_log_prob_[1], index=vectorizer.get_feature_names())\n",
    "    yes_prob.sort(ascending=False)\n",
    "    print('top 10 indicative words for high salary class:')\n",
    "    print(yes_prob[:10])\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "# B4\n",
    "def bigram():\n",
    "    text = str_text()\n",
    "\n",
    "    # this is a function I adapted from http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "    # basically it takes in a string of tokenized words (one job description = one string),\n",
    "    # creates all possible bigrams, and finds the most important bigrams based on a score (feature selection!),\n",
    "    # and returns a string of all unigrams (= words) and bigrams\n",
    "    def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=100):\n",
    "        bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "        bigrams = bigram_finder.nbest(score_fn, n)\n",
    "        # make sure all elements in the list are strings instead of lists, otherwise join() won't take it\n",
    "        bigrams = [str(b) for b in bigrams]\n",
    "        words = [str(w) for w in words]\n",
    "        # merge two lists\n",
    "        words = words + bigrams\n",
    "        # return a string of bigrams and unigrams\n",
    "        return ' '.join(words)\n",
    "\n",
    "    ### IT'S ALL ABOUT DATA STRUCTURE AND DATA TYPE...\n",
    "    # WHAT FEEDS INTO THE X IN FINAL MODEL? a series, and each element should contain all unigrams and bigrams\n",
    "    # so that's what we're gonna do. bigram_word_feats() builds the elements for us, we have to put them into a series now\n",
    "    all_rows_list = []\n",
    "    # the data structure of text is: a tuple, length is 2, element 1 is a series of all job descriptions, element 2 is all salaries\n",
    "    for i in range(len(text[0])):\n",
    "        row_tokenized = nltk.word_tokenize(text[0][i])\n",
    "        liststr = bigram_word_feats(row_tokenized)\n",
    "        all_rows_list.append(liststr)\n",
    "    all_rows_series = pd.Series(all_rows_list)\n",
    "\n",
    "    # build the X matrix\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(all_rows_series)\n",
    "    \n",
    "    \n",
    "    X = X.toarray()\n",
    "\n",
    "    # build y by masking the salary into high (75 percentile and above) and low (below that)\n",
    "    salary = str_text()[1]\n",
    "    cutoff = salary.quantile(q=.75)\n",
    "    y = (salary >= cutoff)\n",
    "    \n",
    "    n = len(y[y==0])\n",
    "    index_high = [i for i in y.index if y[i]]\n",
    "    index_low = [i for i in y.index if not y[i]]\n",
    "    new_high = npr.choice(index_high, size = n, replace = True)\n",
    "    high_X = X[new_high]\n",
    "    high_y = y[new_high]\n",
    "    low_X = X[index_low]\n",
    "    low_y = y[index_low]\n",
    "    new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "    new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "\n",
    "    # split into training and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # fit the model and predict\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print('Multinomial prediction with bigram:')\n",
    "    return conf_matrix, accuracy\n",
    "\n",
    "\n",
    "def tf_idf():\n",
    "    text = str_text()\n",
    "\n",
    "    # this is a function I adapted from http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "    # basically it takes in a string of tokenized words (one job description = one string),\n",
    "    # creates all possible bigrams, and finds the most important bigrams based on a score (feature selection!),\n",
    "    # and returns a string of all unigrams (= words) and bigrams\n",
    "    def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=100):\n",
    "        bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "        bigrams = bigram_finder.nbest(score_fn, n)\n",
    "        # make sure all elements in the list are strings instead of lists, otherwise join() won't take it\n",
    "        bigrams = [str(b) for b in bigrams]\n",
    "        words = [str(w) for w in words]\n",
    "        # merge two lists\n",
    "        words = words + bigrams\n",
    "        # return a string of bigrams and unigrams\n",
    "        return ' '.join(words)\n",
    "\n",
    "    ### IT'S ALL ABOUT DATA STRUCTURE AND DATA TYPE...\n",
    "    # WHAT FEEDS INTO THE X IN FINAL MODEL? a series, and each element should contain all unigrams and bigrams\n",
    "    # so that's what we're gonna do. bigram_word_feats() builds the elements for us, we have to put them into a series now\n",
    "    all_rows_list = []\n",
    "    # the data structure of text is: a tuple, length is 2, element 1 is a series of all job descriptions, element 2 is all salaries\n",
    "    for i in range(len(text[0])):\n",
    "        row_tokenized = nltk.word_tokenize(text[0][i])\n",
    "        liststr = bigram_word_feats(row_tokenized)\n",
    "        all_rows_list.append(liststr)\n",
    "    all_rows_series = pd.Series(all_rows_list)\n",
    "\n",
    "    # build the X matrix\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(all_rows_series)\n",
    "    X = X.toarray()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X = tfidf_transformer.fit_transform(X)\n",
    "    X = X.toarray()\n",
    "\n",
    "    # build y by masking the salary into high (75 percentile and above) and low (below that)\n",
    "    salary = str_text()[1]\n",
    "    cutoff = salary.quantile(q=.75)\n",
    "    y = (salary >= cutoff)\n",
    "    \n",
    "    n = len(y[y==0])\n",
    "    index_high = [i for i in y.index if y[i]]\n",
    "    index_low = [i for i in y.index if not y[i]]\n",
    "    new_high = npr.choice(index_high, size = n, replace = True)\n",
    "    high_X = X[new_high]\n",
    "    high_y = y[new_high]\n",
    "    low_X = X[index_low]\n",
    "    low_y = y[index_low]\n",
    "    new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "    new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "\n",
    "    # split into training and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # fit the model and predict\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print('Multinomial prediction with tf-idf:')\n",
    "    return conf_matrix, accuracy\n",
    "\n",
    "\n",
    "print(prediction())\n",
    "print(prediction_lemma())\n",
    "print(bigram())\n",
    "print(tf_idf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
